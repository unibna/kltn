{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 \n",
    "import gym\n",
    "import time\n",
    "import itertools\n",
    "import codecs\n",
    "import math\n",
    "import os\n",
    "\n",
    "from loguru import logger\n",
    "from keras.models import load_model\n",
    "from gym import Env, spaces\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEBinary():\n",
    "    def __init__(self, bytepath, asmpath):\n",
    "        self.bytepath = bytepath\n",
    "        self.asmpath = asmpath\n",
    "        \n",
    "        self.name = bytepath.split(\"\\\\\")[-1]\n",
    "        self.matrix = None \n",
    "        self.base_address = None\n",
    "        self.inject_locations = None\n",
    "        self.locations_by_section = None\n",
    "        # Number of sections in the binary\n",
    "        self.nsections = None\n",
    "        \n",
    "        self.init_matrix()\n",
    "        self.get_inject_locations()\n",
    "        \n",
    "        self.nsections = self.group_locations()\n",
    "        self.file_stat()\n",
    "        \n",
    "        \n",
    "    def init_matrix(self):\n",
    "        with open(self.bytepath, 'r') as f:\n",
    "            arr = []\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            # Get base_address\n",
    "            self.base_address = int(lines[0].split()[0], 16)\n",
    "            \n",
    "            for line in lines:\n",
    "                vals = line.split()\n",
    "                del vals[0]\n",
    "                arr.append(vals)\n",
    "            \n",
    "            max_len = max([len(vals) for vals in arr])\n",
    "            \n",
    "            new_arr = []\n",
    "            for vals in arr:\n",
    "                new_arr.append([val.replace('?', '0') for val in vals])\n",
    "            \n",
    "            for vals in new_arr:\n",
    "                if '?' in vals:\n",
    "                    print(vals)\n",
    "            \n",
    "            hexstring = ''.join(list(itertools.chain.from_iterable(new_arr)))\n",
    "            \n",
    "            byte_arr = bytearray.fromhex(hexstring)\n",
    "            \n",
    "        \n",
    "            raw_width = math.floor(math.sqrt(len(byte_arr)))\n",
    "            \n",
    "            rem = len(byte_arr) % raw_width\n",
    "            byte_arr_len = len(byte_arr) - rem\n",
    "            byte_arr = byte_arr[:byte_arr_len]\n",
    "            byte_arr = np.asarray(byte_arr)\n",
    "            np_arr = np.reshape(byte_arr, (len(byte_arr)// raw_width, raw_width))\n",
    "            np_arr = np.uint8(np_arr)\n",
    "            \n",
    "            self.matrix = np_arr\n",
    "\n",
    "    \n",
    "    def inject(self, data, value):\n",
    "        offset = data[0] - self.base_address\n",
    "        length = data[1]\n",
    "        # print(\"Injecting at\", offset, \" with length \", length)\n",
    "        self.matrix[offset: offset + length] = value\n",
    "        \n",
    "    def inject_section(self, section, value):\n",
    "        # print(\"Injecting in \", section)\n",
    "        for loc in self.locations_by_section[section]:\n",
    "            self.inject(loc, value)\n",
    "\n",
    "\n",
    "    def get_inject_locations(self):\n",
    "        with codecs.open(self.asmpath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "            results = []\n",
    "            idx = 0\n",
    "            numlines = len(lines)\n",
    "            while idx < numlines:\n",
    "                if lines[idx].find('align ') != -1:\n",
    "                    # print(\"Processing line\", idx, lines[idx])\n",
    "                    section, address = (lines[idx].replace('\\t', ' ').split(' ')[0]).split(':')\n",
    "                    # print(section, address)\n",
    "                    \n",
    "                    found = False\n",
    "                    while not found:\n",
    "                        if idx + 1 < numlines:\n",
    "                            nextline = lines[idx+1]\n",
    "                            _, next_address = nextline.replace('\\t', ' ').split(' ')[0].split(':')\n",
    "                            if address != next_address:\n",
    "                                found = True\n",
    "                                length = int(next_address, 16) - int(address, 16)\n",
    "                                results.append((section, int(address, 16), length))\n",
    "                            idx += 1\n",
    "                        else:\n",
    "                            found = True\n",
    "                idx += 1\n",
    "                \n",
    "            self.inject_locations = results\n",
    "            \n",
    "    def group_locations(self):\n",
    "        locations = sorted(self.inject_locations, key=lambda x: x[2], reverse=True)\n",
    "        d = defaultdict(list)\n",
    "        \n",
    "        for name, *v in locations:\n",
    "            d[name].append(v)    \n",
    "            \n",
    "        self.locations_by_section = d\n",
    "        \n",
    "        return len(self.locations_by_section.keys())\n",
    "    \n",
    "    def file_stat(self):\n",
    "        print(\"File: \", self.bytepath)\n",
    "        for sec in self.locations_by_section:\n",
    "            print(\"Section: {} | Largest size: {}\".format(sec, self.locations_by_section[sec][0][1]))\n",
    "        \n",
    "    \n",
    "\n",
    "class InjectorEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(InjectorEnv, self).__init__()\n",
    "\n",
    "        self.model = load_model(\"C:\\\\Users\\\\hao.le\\\\Documents\\\\Projects\\\\Thesis\\\\kltn\\\\model\\\\checkpoint.h5\")\n",
    "\n",
    "        self.bytefolder = \"D:\\\\Big2015\\\\dataset-10-8-2\\\\8-2-0.0\\\\train-bytes\\\\1\"\n",
    "        self.asmfolder = \"D:\\\\Big2015\\\\dataset-10-8-2\\\\8-2-0.0\\\\8.2.0.0-asm\\\\train-asm\\\\1\"\n",
    "        \n",
    "        self.bytes = []\n",
    "        self.asms = []\n",
    "        \n",
    "        self.action_space = spaces.MultiDiscrete([5, 255])\n",
    "        \n",
    "        self.observation_shape = (256, 256)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=self.observation_shape, dtype=np.uint8)\n",
    "        \n",
    "        self.max_injections = 50\n",
    "        self.injections_left = 50\n",
    "        self.current_file_idx = -1\n",
    "        self.PEBinary = None\n",
    "        \n",
    "        self.canvas = np.full(self.observation_shape, 255, dtype=np.uint8)\n",
    "        self.nsections = 0\n",
    "        \n",
    "        self.load_input()\n",
    "\n",
    "    def load_input(self):\n",
    "        for file in os.listdir(self.bytefolder):\n",
    "            if file.endswith(\".bytes\"):\n",
    "                path = os.path.join(self.bytefolder, file)\n",
    "                self.bytes.append(path)\n",
    "                \n",
    "        for file in os.listdir(self.asmfolder):\n",
    "            if file.endswith(\".asm\"):\n",
    "                path = os.path.join(self.asmfolder, file)\n",
    "                self.asms.append(path)\n",
    "                \n",
    "        self.next_file()\n",
    "        \n",
    "        \n",
    "    def next_file(self):\n",
    "        if self.current_file_idx >= len(self.bytes):\n",
    "            self.current_file_idx = 0\n",
    "        else:\n",
    "            self.current_file_idx += 1\n",
    "        self.PEBinary = PEBinary(self.bytes[self.current_file_idx],\n",
    "                                 self.asms[self.current_file_idx])\n",
    "        self.nsections = self.PEBinary.nsections\n",
    "        print(\"Binary: \", self.PEBinary.bytepath.split()[-1])\n",
    "        \n",
    "        self.update_action_space()\n",
    "    \n",
    "    def update_action_space(self):\n",
    "        # Update action space to accommodate the number of sections to inject\n",
    "        self.action_space = spaces.MultiDiscrete([self.nsections, 255])\n",
    "        \n",
    "    def predict_binary(self):\n",
    "        img_array = np.stack((self.canvas,)*3, axis=2) # (256, 256, 3)\n",
    "        img_batch = np.expand_dims(img_array, axis=0)\n",
    "        img_preprocessed = preprocess_input(img_batch)\n",
    "        \n",
    "        prediction = self.model.predict(img_preprocessed)\n",
    "        print(prediction)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        assert self.action_space.contains(action), \"Invalid Action\"\n",
    "        \n",
    "        self.injections_left -= 1\n",
    "        \n",
    "        if self.injections_left == 0:\n",
    "            done = True\n",
    "        \n",
    "        section_idx = action[0]\n",
    "        code = action[1]\n",
    "        \n",
    "        keys = list(self.PEBinary.locations_by_section.keys())\n",
    "        if section_idx < len(keys):\n",
    "            section = keys[section_idx]\n",
    "            self.PEBinary.inject_section(section, code)\n",
    "        else:\n",
    "            reward = -10\n",
    "            \n",
    "        self.update_canvas()\n",
    "        \n",
    "        self.predict_binary()\n",
    "        \n",
    "        return self.canvas, reward, done, {}\n",
    "    \n",
    "    def update_canvas(self):\n",
    "        self.img = utils.np2img(self.PEBinary.matrix, 256, 256)\n",
    "        self.canvas = np.asarray(self.img)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the number of injections left\n",
    "        self.injections_left = self.max_injections\n",
    "        \n",
    "        # Get next input file\n",
    "        self.next_file()\n",
    "        \n",
    "        # Map PE Binary to canvas (256 x 256)\n",
    "        self.update_canvas()\n",
    "\n",
    "        # # Draw elements on the canvas\n",
    "        # self.draw_canvas()\n",
    "        \n",
    "        return self.canvas  # reward, done, info can't be included\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        assert mode in [\"human\", \"rgb_array\"], \"Invalid mode, must be either \\\"human\\\" or \\\"rgb_array\\\"\"\n",
    "        if mode == \"human\":\n",
    "            # stacked = np.stack((self.canvas,)*3, axis=2)\n",
    "            cv2.imshow(self.PEBinary.name, self.canvas)\n",
    "            cv2.waitKey(10)\n",
    "        \n",
    "        elif mode == \"rgb_array\":\n",
    "            return self.canvas\n",
    "\n",
    "    def close (self):\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:  D:\\Big2015\\dataset-10-8-2\\8-2-0.0\\train-bytes\\1\\0odUVkrjp2B1n8NDS6bR.bytes\n",
      "Section: .bss | Largest size: 5630\n",
      "Section: .rdata | Largest size: 3584\n",
      "Section: .data | Largest size: 1148\n",
      "Section: .text | Largest size: 15\n",
      "Binary:  D:\\Big2015\\dataset-10-8-2\\8-2-0.0\\train-bytes\\1\\0odUVkrjp2B1n8NDS6bR.bytes\n",
      "File:  D:\\Big2015\\dataset-10-8-2\\8-2-0.0\\train-bytes\\1\\0ZiQmgtxzHe9v5O8Lf2k.bytes\n",
      "Section: .data | Largest size: 2536\n",
      "Section: .rdata | Largest size: 1086\n",
      "Section: .text | Largest size: 70\n",
      "Binary:  D:\\Big2015\\dataset-10-8-2\\8-2-0.0\\train-bytes\\1\\0ZiQmgtxzHe9v5O8Lf2k.bytes\n"
     ]
    }
   ],
   "source": [
    "bytepath = \"./dataSample/0A32eTdBKayjCWhZqDOQ.bytes\"\n",
    "asmpath = \"./dataSample/0A32eTdBKayjCWhZqDOQ.asm\"\n",
    "env = InjectorEnv()\n",
    "obs = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hao.le\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:130: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:  D:\\Big2015\\dataset-10-8-2\\8-2-0.0\\train-bytes\\1\\29NR1zBEDCPM5xntsdlA.bytes\n",
      "Section: .data | Largest size: 3420\n",
      "Section: .text | Largest size: 212\n",
      "Binary:  D:\\Big2015\\dataset-10-8-2\\8-2-0.0\\train-bytes\\1\\29NR1zBEDCPM5xntsdlA.bytes\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common import env_checker\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:  D:\\Big2015\\dataset-10-8-2\\8-2-0.0\\train-bytes\\1\\2F6ZfVCQRi3vrwcj4zxL.bytes\n",
      "Section: .data | Largest size: 3800\n",
      "Section: .text | Largest size: 376\n",
      "Section: .rdata | Largest size: 14\n",
      "Binary:  D:\\Big2015\\dataset-10-8-2\\8-2-0.0\\train-bytes\\1\\2F6ZfVCQRi3vrwcj4zxL.bytes\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "Episode:1 Score:0\n",
      "File:  D:\\Big2015\\dataset-10-8-2\\8-2-0.0\\train-bytes\\1\\2fl8q7dsoC1PwApx6QNg.bytes\n",
      "Section: .rdata | Largest size: 4072\n",
      "Section: DATA | Largest size: 3068\n",
      "Section: BSS | Largest size: 2092\n",
      "Section: CODE | Largest size: 295\n",
      "Binary:  D:\\Big2015\\dataset-10-8-2\\8-2-0.0\\train-bytes\\1\\2fl8q7dsoC1PwApx6QNg.bytes\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hao.le\\Documents\\Projects\\Thesis\\kltn\\RL\\test.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hao.le/Documents/Projects/Thesis/kltn/RL/test.ipynb#ch0000004?line=10'>11</a>\u001b[0m         n_state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hao.le/Documents/Projects/Thesis/kltn/RL/test.ipynb#ch0000004?line=11'>12</a>\u001b[0m         score\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mreward\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hao.le/Documents/Projects/Thesis/kltn/RL/test.ipynb#ch0000004?line=12'>13</a>\u001b[0m         time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.5\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hao.le/Documents/Projects/Thesis/kltn/RL/test.ipynb#ch0000004?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpisode:\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m Score:\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(episode, score))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hao.le/Documents/Projects/Thesis/kltn/RL/test.ipynb#ch0000004?line=14'>15</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        time.sleep(0.5)\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c0fc99923d193d75a027b6b50e44bb06e8c0f060e9a8926d733d34b837b3d9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
